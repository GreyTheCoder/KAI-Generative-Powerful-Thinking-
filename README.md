KAI ‚Äî Generative Powerful Thinking ü§ñ
KAI is a full-stack AI Chat Assistant built to bridge the gap between powerful LLMs and a custom, user-centric interface. This project was developed to master the technical "plumbing" of modern AI applications, focusing on real-time streaming, conversation persistence, and high-fidelity technical rendering. 

üîó Live Deployments
Frontend: https://kai-generative-powerful-thinking-frontend.onrender.com

Backend API: https://kai-generative-powerful-thinking-backend.onrender.com

https://github.com/user-attachments/assets/f8041eb7-3c92-49a2-928e-2ce0ec70ecd1
## What it does
- Chat with OpenAI models from a custom UI
- Streams responses so the assistant appears to ‚Äútype‚Äù in real time
- Renders Markdown and syntax-highlighted code blocks (rehype-highlight + react-markdown)
- Persists conversations in MongoDB and keeps sessions separate using UUIDs

## Why I built it
I wanted a hands-on way to learn streaming responses, manage conversational state, and handle code blocks correctly in the browser. It‚Äôs small but packed with practical patterns I keep reaching for in production apps.

## Tech
- Frontend: React 19 + Vite, react-markdown, rehype-highlight, react-spinners
- Backend: Node, Express, Mongoose (MongoDB), OpenAI SDK, dotenv
- Utilities: uuid for session IDs

## Quick start
1. Clone
   git clone https://github.com/GreyTheCoder/KAI-Generative-Powerful-Thinking-.git

2. Backend
   cd backend
   npm install



   create a `.env`:
   ```
   PORT=5000
   MONGO_URI=your_mongo_uri
   OPENAI_API_KEY=your_openai_key
   ```
   npm start

3. Frontend
   cd frontend
   npm install
   npm run dev
   Open the Vite URL (usually http://localhost:5173)

## Notes on implementation
- Streaming: backend proxies OpenAI streaming responses to the frontend so the UI can update incrementally.
- Markdown & code: react-markdown + rehype-highlight for safe, readable output.
- Persistence: messages saved with Mongoose (author, text, timestamp, sessionId) so you can resume conversations later.

## Challenges I ran into
- Making streaming feel smooth without re-render thrash ‚Äî solved with small buffering and throttled UI updates.
- Handling token limits ‚Äî solved by pruning or summarizing older context before sending.
- Keeping the OpenAI key secure ‚Äî always call the API from the backend, never expose keys to the client.

## What I learned
Streaming changes how you design the UI: incremental rendering and careful state updates matter. Small UX details (loading indicators, message pacing) make the chat feel alive. Also got better at balancing model costs vs. user context.


